{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import ConvMixer, MlpMixer, ViT\n",
    "from torchvision.datasets import CIFAR10, ImageFolder\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.empty_cache())\n",
    "# print(torch.cuda.memory_summary(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "hdim=1024\n",
    "depth=32\n",
    "\n",
    "epochs=1\n",
    "\n",
    "scale=0.75\n",
    "reprob=0.25\n",
    "ra_m=8\n",
    "ra_n=1\n",
    "jitter=0.1\n",
    "psize=2\n",
    "conv_ks=5\n",
    "wd=0.01\n",
    "clip_norm=True\n",
    "lr_max=0.01\n",
    "workers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "if not Path('data/tiny-imagenet-200').exists():\n",
    "    os.system('wget http://cs231n.stanford.edu/tiny-imagenet-200.zip -P data')\n",
    "    os.system('unzip -qq data/tiny-imagenet-200.zip -d data')\n",
    "\n",
    "DATA_DIR = 'data/tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'val')\n",
    "\n",
    "traindata = ImageFolder(TRAIN_DIR, transform=T.Compose([\n",
    "    T.RandomResizedCrop(64, scale=(scale, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "]))\n",
    "\n",
    "valdata = ImageFolder(VALID_DIR, transform=T.Compose([\n",
    "    T.Resize(64),\n",
    "    T.CenterCrop(64),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "]))\n",
    "\n",
    "trainloader = DataLoader(traindata, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "valloader = DataLoader(valdata, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "\n",
    "print(len(traindata))\n",
    "print(len(valdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "# cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "# train_transform = T.Compose([\n",
    "#     T.RandomResizedCrop(32, scale=(scale, 1.0), ratio=(1.0, 1.0)),\n",
    "#     T.RandomHorizontalFlip(p=0.5),\n",
    "#     T.RandAugment(num_ops=ra_n, magnitude=ra_m),\n",
    "#     T.ColorJitter(jitter, jitter, jitter),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(cifar10_mean, cifar10_std),\n",
    "#     T.RandomErasing(p=reprob)\n",
    "# ])\n",
    "\n",
    "# test_transform = T.Compose([\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(cifar10_mean, cifar10_std)\n",
    "# ])\n",
    "# traindata = CIFAR10(root=\"data\", train=True, download=True, transform=train_transform)\n",
    "# testdata = CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "# trainloader = DataLoader(traindata, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "# testloader = DataLoader(testdata, batch_size=batch_size, shuffle=False, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(model, get_time=True, record_time_len=10, verbose=False):\n",
    "    opt = optim.AdamW(model.parameters(), lr=lr_max, weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    max_mem = 0.\n",
    "    transfered = []\n",
    "    step_time = []\n",
    "    record_transfers = 2\n",
    "    record_mem = 3\n",
    "    record_time = list(range(4, 4+record_time_len))\n",
    "    end_step = max(4+record_time_len, record_mem) if get_time else record_mem\n",
    "    if verbose:\n",
    "        print(f\"batch_size: {batch_size}, width: {model.width}, depth: {model.depth}\")\n",
    "        print(f\"Total params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"Total params size in gb: {sum(p.element_size()*p.nelement() for p in model.parameters())/1024**3:.4f} GB\")\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        if i in record_time: start_step = time.time()\n",
    "        if i == record_mem: torch.cuda.reset_peak_memory_stats()\n",
    "        model.train()\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        if i == record_transfers:\n",
    "            transfered.append(X.element_size() * X.nelement())\n",
    "            transfered.append(y.element_size() * y.nelement())\n",
    "\n",
    "        # lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
    "        # opt.param_groups[0].update(lr=lr)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        if i == record_transfers:\n",
    "            transfered.append(loss.element_size() * loss.nelement())\n",
    "            transfered.append(model(X, get_transfer=True))\n",
    "\n",
    "        loss.backward()\n",
    "        if clip_norm:\n",
    "            # scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # scaler.step(opt)\n",
    "        # scaler.update()\n",
    "        opt.step()\n",
    "        # print(f\"step {i} of {len(trainloader)}\")\n",
    "        if i in record_time: step_time.append(time.time() - start_step)\n",
    "        if i == record_mem: max_mem = torch.cuda.max_memory_allocated(0)/1024**3\n",
    "        if verbose:\n",
    "            if i == record_time[-1]: print(f'avg step time: {np.mean(step_time):.4f} +- {np.std(step_time)}s')\n",
    "            if i == record_mem:\n",
    "                print(f'max_mem: {max_mem:.4f} GB')\n",
    "            if i == record_transfers:\n",
    "                print(f'transfered: {np.array(transfered).mean()/1024:.4f} kB')\n",
    "        if i == end_step: break\n",
    "    del model\n",
    "    return max_mem, transfered, step_time\n",
    "\n",
    "_ = get_stats(ResMlp(256, 1024, 2**12*3, 1000).cuda(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(mem_limit, hdim=1024, gch=False, offload=False):\n",
    "    # binary search to find depth to fit in mem_limit\n",
    "    init_alloc = torch.cuda.memory_allocated(0)/1024**3\n",
    "    # print(f\"memory allocated before model: {init_alloc}GB\")\n",
    "    def get_stats_closure(depth, hdim, gch=gch, offload=offload):\n",
    "        if offload:\n",
    "            model = ResMlp(depth, hdim, 2**12*3, 1000, offload=True)\n",
    "        else:\n",
    "            model = ResMlp(depth, hdim, 2**12*3, 1000, gch=gch).cuda()\n",
    "        return get_stats(model, get_time=False)[0] - init_alloc\n",
    "        return get_stats(MlpMixer(num_blocks=depth, embed_dim=hdim, grad_checkpointing=gch).cuda(), get_time=False)[0] - init_alloc\n",
    "    depth = 1\n",
    "    while True:\n",
    "        forward_mem = get_stats_closure(depth, hdim, gch)\n",
    "        # print(f\"depth: {depth}, hdim: {hdim}, forward_mem: {forward_mem:.4f}GB\")\n",
    "        if forward_mem > mem_limit:\n",
    "            if depth == 1:\n",
    "                hdim //= 2\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        depth *= 2\n",
    "    scale = depth // 2\n",
    "    depth -= scale\n",
    "    while True:\n",
    "        forward_mem = get_stats_closure(depth, hdim, gch)\n",
    "        # print(f\"depth: {depth}, hdim: {hdim}, forward_mem: {forward_mem:.4f}GB\")\n",
    "        scale //= 2\n",
    "        if forward_mem > mem_limit: depth -= scale\n",
    "        else: depth += scale\n",
    "        if scale == 1: break\n",
    "    forward_mem = get_stats_closure(depth, hdim, gch)\n",
    "    # print(f\"depth: {depth}, hdim: {hdim}, forward_mem: {forward_mem:.4f}GB\")\n",
    "    if forward_mem > mem_limit: depth -= 1\n",
    "    if offload:\n",
    "        model = ResMlp(depth, hdim, 2**12*3, 1000, offload=True)\n",
    "    else:\n",
    "        model = ResMlp(depth, hdim, 2**12*3, 1000, gch=gch).cuda()\n",
    "    return depth, hdim, get_stats(model, get_time=True), init_alloc\n",
    "\n",
    "def process_stats(out):\n",
    "    depth,hdim,stats,init_alloc = out\n",
    "    max_mem, transfered, step_time = stats\n",
    "    return depth, hdim,max_mem - init_alloc, f\"{np.array(transfered).sum()/1024} Kb\", f\"{np.mean(step_time)} ± {np.std(step_time)}\"\n",
    "\n",
    "print(\"mem_budget: 1gb, baseline\", process_stats(get_model_size(1)))\n",
    "print(\"mem_budget: 1gb, checkpointing\", process_stats(get_model_size(1, gch=True)))\n",
    "print(\"mem_budget: 1gb, offloading\", process_stats(get_model_size(1, offload=True)))\n",
    "print()\n",
    "\n",
    "# print(\"mem_budget: 2gb, baseline\", process_stats(get_model_size(2)))\n",
    "# print(\"mem_budget: 2gb, checkpointing\", process_stats(get_model_size(2, gch=True)))\n",
    "# print(\"mem_budget: 2gb, offloading\", process_stats(get_model_size(2, offload=True)))\n",
    "# print()\n",
    "\n",
    "# print(\"mem_budget: 4gb, baseline\", process_stats(get_model_size(4)))\n",
    "# print(\"mem_budget: 4gb, checkpointing\", process_stats(get_model_size(4, gch=True)))\n",
    "# print(\"mem_budget: 4gb, offloading\", process_stats(get_model_size(4, offload=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the latency to move data from CPU to GPU, and GPU to CPU\n",
    "print(\"Measuring data transfer latency...\")\n",
    "bandwidth = []\n",
    "trange = np.arange(1,21)\n",
    "for i in trange:\n",
    "    data_amt = 1 << i\n",
    "    data = torch.randn(data_amt)\n",
    "    print(f\"Data amount: {data_amt} = {data_amt*data.element_size()/2**20:.4f}MB = 2^{i} bytes\")\n",
    "    cudata = data.cuda()\n",
    "    for _ in range(10):\n",
    "        cudata.mean().cpu().numpy()\n",
    "    baseline = np.array(timeit.repeat(lambda: cudata.mean().cpu().numpy(), number=10000, repeat=7)) / 10000\n",
    "    for _ in range(10):\n",
    "        data.cuda().mean().cpu().numpy()\n",
    "    real = np.array(timeit.repeat(lambda: data.cuda().mean().cpu().numpy(), number=10000, repeat=7)) / 10000\n",
    "    bandwidth.append((data_amt * data.element_size())/(2**20 * (real-baseline)))\n",
    "    print(f\"bandwidth: {np.mean(bandwidth[-1]):.4e} ± {np.std(bandwidth[-1]):.4e} Mb/s\")\n",
    "    del data, cudata\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "npband = np.array(bandwidth)\n",
    "plt.plot(4*np.float32(2)**trange/2**20, npband.mean(1), label=\"mean\")\n",
    "plt.fill_between(4*np.float32(2)**trange/2**20, npband.mean(1)-npband.std(1), npband.mean(1)+npband.std(1), alpha=0.5, label=\"std\")\n",
    "plt.xlabel(\"Data amount Mb\")\n",
    "plt.ylabel(\"Bandwidth (Mb/s)\")\n",
    "plt.savefig(\"bandwidth_lut.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_alloc = torch.cuda.memory_allocated(0)/1024**3\n",
    "init_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 14\n",
    "hdim = 1024\n",
    "# model = ConvMixer(hdim, depth, kernel_size=9, patch_size=7, n_classes=1000)\n",
    "model = MlpMixer(num_blocks=depth, embed_dim=hdim).cuda() \n",
    "lr_schedule = lambda t: np.interp([t], [0, epochs*2//5, epochs*4//5, epochs], \n",
    "                                  [0, lr_max, lr_max/20.0, 0])[0]\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=lr_max, weight_decay=wd)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"batch_size: {batch_size}, hdim: {hdim}, depth: {depth}\")\n",
    "print(f\"Total params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Total params size in gb: {sum(p.element_size()*p.nelement() for p in model.parameters())/1024**3:.4f}GB\")\n",
    "max_mem = 0.\n",
    "transfered = 0\n",
    "step_time = []\n",
    "record_mem = 3\n",
    "record_time = list(range(4, 4+100))\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc, n = 0, 0, 0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        if i in record_time: start_step = time.time()\n",
    "        if i == record_mem: torch.cuda.reset_peak_memory_stats(0)\n",
    "        model.train()\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        if i == record_mem:\n",
    "            transfered += X.element_size() * X.nelement()\n",
    "            transfered += y.element_size() * y.nelement()\n",
    "\n",
    "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
    "        opt.param_groups[0].update(lr=lr)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        if i == record_mem:\n",
    "            transfered += loss.element_size() * loss.nelement()\n",
    "\n",
    "        loss.backward()\n",
    "        if clip_norm:\n",
    "            # scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # scaler.step(opt)\n",
    "        # scaler.update()\n",
    "        opt.step()\n",
    "\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "        train_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "        # print(f\"step {i} of {len(trainloader)}\")\n",
    "        if i in record_time: step_time.append(time.time() - start_step)\n",
    "        if i == record_time[-1]: print(f'avg step time: {np.mean(step_time):.4f} +- {np.std(step_time)}s')\n",
    "\n",
    "        if i == record_mem:\n",
    "            max_mem = torch.cuda.max_memory_allocated(0)/1024**3\n",
    "            print(f'max_mem: {max_mem:.4f} GB, transfered: {transfered/1024**2:.4f} MB')\n",
    "        \n",
    "    model.eval()\n",
    "    test_acc, m = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(X)\n",
    "            test_acc += (output.max(1)[1] == y).sum().item()\n",
    "            m += y.size(0)\n",
    "\n",
    "    print(f'[ConvMixer] Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from models import Residual\n",
    "from torch.utils import checkpoint\n",
    "\n",
    "class OffloadCheckpoint(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, fn, x):\n",
    "        with torch.no_grad():\n",
    "            y = fn.cuda()(x)\n",
    "        ctx.run_function = fn.cpu()\n",
    "        ctx.inp = x.detach().cpu()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        x = ctx.inp\n",
    "        z = x.detach()\n",
    "        z.requires_grad = True\n",
    "        x = z\n",
    "        x = x.cuda()\n",
    "        x.requires_grad = True\n",
    "        with torch.enable_grad():\n",
    "            outputs = ctx.run_function.cuda()(x)\n",
    "        grad = grad.cuda()\n",
    "        torch.autograd.backward(outputs, grad)\n",
    "        ctx.run_function.cpu()\n",
    "        x_grad = x.grad.to(\"cuda:0\")\n",
    "        return None, x_grad\n",
    "\n",
    "\n",
    "Block = lambda hdim: Residual(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(hdim, hdim),\n",
    "        nn.GELU(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "class ResMlp(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, width, input_dim, out_dim, gch=False, offload=False):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.stem = nn.Sequential(nn.Flatten(), nn.Linear(input_dim, width))\n",
    "        self.gch = gch\n",
    "        self.offload = offload\n",
    "\n",
    "        chunk_size = int(math.sqrt(depth))\n",
    "        num_chunks = depth // chunk_size\n",
    "        leftover = depth % chunk_size\n",
    "        if self.gch or self.offload:\n",
    "            self.blocks = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    *[Block(width) for _ in range(chunk_size)]\n",
    "                ) for _ in range(num_chunks)\n",
    "            ] + [nn.Sequential(\n",
    "                    *[Block(width) for _ in range(leftover)]\n",
    "            )])\n",
    "        else:\n",
    "            self.blocks = nn.ModuleList([Block(width) for _ in range(depth)])\n",
    "        self.head = nn.Linear(width, out_dim)\n",
    "    \n",
    "    def forward(self, x, get_transfer=False):\n",
    "        transfered = 0\n",
    "        if self.offload:\n",
    "            x = self.stem.cuda()(x)\n",
    "            for block in self.blocks:\n",
    "                with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "                    x = OffloadCheckpoint.apply(block, x)\n",
    "                if get_transfer:\n",
    "                    transfered += x.element_size() * x.nelement() + sum([p.element_size() * p.nelement() for p in block.parameters()])\n",
    "            x = self.head.cuda()(x)\n",
    "            if get_transfer:\n",
    "                return transfered\n",
    "        else:\n",
    "            if get_transfer:\n",
    "                return transfered\n",
    "            x = self.stem(x)\n",
    "            if self.gch:\n",
    "                for block in self.blocks:\n",
    "                    x = checkpoint.checkpoint(block, x)\n",
    "            else:\n",
    "                for block in self.blocks:\n",
    "                    x = block(x)\n",
    "            x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 498\n",
    "width = 1024\n",
    "print(f\"depth: {depth}, width: {width}\")\n",
    "\n",
    "X,y = next(iter(trainloader))\n",
    "X = X.cuda()\n",
    "\n",
    "def test_model(name, mod_model, init_alloc, baseline=None):\n",
    "    print(name)\n",
    "    print(f\"init alloc: {init_alloc/1024**3:.4f} GB\")\n",
    "    if baseline is not None:\n",
    "        for p, q in zip(baseline.parameters(), mod_model.parameters()):\n",
    "            q.data = p.data.to(q.device)\n",
    "    torch.cuda.reset_peak_memory_stats(0)\n",
    "    loss = nn.CrossEntropyLoss()(mod_model(X), y.cuda())\n",
    "    loss.backward()\n",
    "    max_mem = torch.cuda.max_memory_allocated(0) - init_alloc\n",
    "    print(f\"used: {max_mem/2**30} GB\")\n",
    "    res = [p.grad.clone().cpu() for p in mod_model.parameters()]\n",
    "    warmup = timeit.timeit(lambda: nn.CrossEntropyLoss()(mod_model(X), y.cuda()).backward, number=10)\n",
    "    speed = timeit.timeit(lambda: nn.CrossEntropyLoss()(mod_model(X), y.cuda()).backward, number=10)/10\n",
    "    print(f\"speed: {speed:.4f} s\")\n",
    "    transfered = mod_model(X, get_transfer=True) + X.element_size() * X.nelement() + y.element_size() * y.nelement() + loss.element_size() * loss.nelement()\n",
    "    print(f\"transfered: {transfered/2**20} MB\")\n",
    "    if baseline:\n",
    "        del mod_model, p, q, loss\n",
    "    else:\n",
    "        del loss\n",
    "    print()\n",
    "    return res\n",
    "\n",
    "init_alloc = torch.cuda.memory_allocated(0)\n",
    "baseline = ResMlp(depth, width, 2**12*3, 1000).cuda()\n",
    "base = test_model(\"Baseline\", baseline, init_alloc)\n",
    "\n",
    "print(\"Naive\")\n",
    "init_alloc = torch.cuda.memory_allocated(0)\n",
    "print(f\"init alloc: {init_alloc/1024**3:.4f} GB\")\n",
    "model_naive = ResMlp(depth, width, 2**12*3, 1000)\n",
    "for p, q in zip(baseline.parameters(), model_naive.parameters()):\n",
    "    q.data = p.data.cpu()\n",
    "torch.cuda.reset_peak_memory_stats(0)\n",
    "loss = nn.CrossEntropyLoss()(model_naive(X.cpu()), y)\n",
    "loss.backward()\n",
    "max_mem = torch.cuda.max_memory_allocated(0) - init_alloc\n",
    "print(f\"used: {max_mem/2**30} GB\")\n",
    "speed = timeit.timeit(lambda: nn.CrossEntropyLoss()(model_naive(X.cpu()), y).backward, number=10)/10\n",
    "print(f\"speed: {speed:.4f} s\")\n",
    "naive = [p.grad.clone().cpu() for p in model_naive.parameters()]\n",
    "del model_naive, p, q, loss\n",
    "print()\n",
    "\n",
    "init_alloc = torch.cuda.memory_allocated(0)\n",
    "gch = test_model(\"Gradient Checkpointing\", ResMlp(depth, width, 2**12*3, 1000, gch=True).cuda(), init_alloc, baseline=baseline)\n",
    "init_alloc = torch.cuda.memory_allocated(0)\n",
    "offload = ResMlp(depth, width, 2**12*3, 1000, offload=True)\n",
    "offload = test_model(\"Gradient Checkpointing with Offload\", offload, init_alloc, baseline=baseline)\n",
    "\n",
    "# init_alloc = torch.cuda.memory_allocated(0)\n",
    "# bigger = ResMlp(114, width, 2**12*3, 1000, offload=True)\n",
    "# test_model(\"Gradient Checkpointing with Offload Bigger\", bigger, init_alloc)\n",
    "# del bigger\n",
    "\n",
    "print(all([torch.allclose(base[i], gch[i]) for i in range(len(base))]))\n",
    "print(all([torch.allclose(base[i], offload[i]) for i in range(len(base))]))\n",
    "del baseline\n",
    "del X, y\n",
    "print(torch.cuda.memory_allocated(0)/2**30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = {\n",
    "\"1GB\": {\n",
    "    # \"depth\": 114, \"width\": 1024,\n",
    "    \"Baseline\": {\n",
    "        \"used\": 0.9930810928344727,\n",
    "        \"speed\": 0.0070,\n",
    "        \"transferred\": 0.09376907348632812,\n",
    "    },\n",
    "\n",
    "    \"CPU Only\": {\n",
    "        \"used\": 0.0,\n",
    "        \"speed\": 0.0220,\n",
    "        \"transferred\": 0.0\n",
    "    },\n",
    "\n",
    "    \"Gradient Checkpointing\": {\n",
    "        \"used\": 0.49654483795166016,\n",
    "        \"speed\": 0.0067,\n",
    "        \"transferred\": 0.09376907348632812,\n",
    "    },\n",
    "\n",
    "    \"Gradient Checkpointing with Offload\": {\n",
    "        \"used\": 0.13294696807861328,\n",
    "        \"speed\": 0.2174,\n",
    "        \"transferred\": 456.6328315734863,\n",
    "    },\n",
    "},\n",
    "\n",
    "\"2GB\": {\n",
    "    # \"depth\": 241, \"width\": 1024,\n",
    "    \"Baseline\": {\n",
    "        \"used\": 1.9862375259399414,\n",
    "        \"speed\": 0.0145,\n",
    "        \"transferred\": 0.09376907348632812,\n",
    "    },\n",
    "\n",
    "    \"CPU Only\": {\n",
    "        \"used\": 0.0,\n",
    "        \"speed\": 0.0441,\n",
    "        \"transferred\": 0.0,\n",
    "    },\n",
    "\n",
    "    \"Gradient Checkpointing\": {\n",
    "        \"used\": 0.9931230545043945,\n",
    "        \"speed\": 0.0131,\n",
    "        \"transferred\": 0.09376907348632812,\n",
    "    },\n",
    "\n",
    "    \"Gradient Checkpointing with Offload\": {\n",
    "        \"used\": 0.17204761505126953,\n",
    "        \"speed\": 0.4588,\n",
    "        \"transferred\": 965.1679878234863,\n",
    "    },\n",
    "},\n",
    "\n",
    "\"4GB\": {\n",
    "    # \"depth\": 498, \"width\": 1024,\n",
    "    \"Baseline\": {\n",
    "        \"used\": 3.9960107803344727,\n",
    "        \"speed\": 0.0300,\n",
    "        \"transferred\": 0.09376907348632812,\n",
    "    },\n",
    "\n",
    "    \"CPU Only\": {\n",
    "        \"used\": 0.0,\n",
    "        \"speed\": 0.0888,\n",
    "        \"transferred\": 0.0,\n",
    "    },\n",
    "\n",
    "    \"Gradient Checkpointing\": {\n",
    "        \"used\": 1.9980096817016602,\n",
    "        \"speed\": 0.0264,\n",
    "        \"transferred\": 0.09376907348632812,\n",
    "    },\n",
    "\n",
    "    \"Gradient Checkpointing with Offload\": {\n",
    "        \"used\": 0.22678852081298828,\n",
    "        \"speed\": 0.9462,\n",
    "        \"transferred\": 1994.2187690734863,\n",
    "    },\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat({k: pd.DataFrame(v).T for k, v in df.items()}, axis=0).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e3522f8c0d66f11a66b445b95395b0fa8bbeadf7a1b18599864fbc55528010c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
